{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bedfefc-2094-4124-8413-9634f2fcf2f8",
   "metadata": {},
   "source": [
    "### Loading data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6858839c-5b0c-4198-b06f-faba87288002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           en_tokens  \\\n",
      "0  [Give, your, application, an, accessibility, w...   \n",
      "1              [Accerciser, Accessibility, Explorer]   \n",
      "2  [The, default, plugin, layout, for, the, botto...   \n",
      "3  [The, default, plugin, layout, for, the, top, ...   \n",
      "4  [A, list, of, plugins, that, are, disabled, by...   \n",
      "\n",
      "                                           hi_tokens  \n",
      "0  [अपने, अनुप्रयोग, को, पहुंचनीयता, व्यायाम, का,...  \n",
      "1                [एक्सेर्साइसर, पहुंचनीयता, अन्वेषक]  \n",
      "2      [निचले, पटल, के, लिए, डिफोल्ट, प्लग-इन, खाका]  \n",
      "3       [ऊपरी, पटल, के, लिए, डिफोल्ट, प्लग-इन, खाका]  \n",
      "4  [उन, प्लग-इनों, की, सूची, जिन्हें, डिफोल्ट, रू...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Your existing data loading code\n",
    "data = []\n",
    "\n",
    "with open('parallel-n/IITB.en-hi.en', 'r', encoding='utf-8') as f_en, \\\n",
    "     open('parallel-n/IITB.en-hi.hi', 'r', encoding='utf-8') as f_hi:\n",
    "\n",
    "    for en_line, hi_line in zip(f_en, f_hi):\n",
    "        en_tokens = en_line.strip().split()\n",
    "        hi_tokens = hi_line.strip().split()\n",
    "        data.append((en_tokens, hi_tokens))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=['en_tokens', 'hi_tokens'])\n",
    "\n",
    "# Show the first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c385a02-213d-4c8e-a7b9-b20218ffc8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1659083, 2)\n"
     ]
    }
   ],
   "source": [
    "# size of dataset\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7698088-9af1-4d18-8881-1afca8a78cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower casing the english records\n",
    "df['en_tokens'] = df['en_tokens'].apply(lambda tokens: [token.lower() for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5d584cf-6a1c-462e-a093-755eb549f3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting length of each records\n",
    "df['en_length'] = df['en_tokens'].apply(len)\n",
    "df['hi_length'] = df['hi_tokens'].apply(len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884cf7c7-a592-4de9-acbf-6102aaa1f683",
   "metadata": {},
   "source": [
    "#### Length wise distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "525fa2db-26c8-443a-a01e-76870354ff50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length Range |  English |    Hindi\n",
      "--------------------------------\n",
      "0-5        |   593705 |   578703\n",
      "5-10       |   281225 |   268038\n",
      "10-15      |   234844 |   225632\n",
      "15-20      |   167756 |   166374\n",
      "20-25      |   121542 |   122311\n",
      "25-30      |    84216 |    88680\n",
      "30-35      |    56216 |    61801\n",
      "35-40      |    36188 |    40461\n",
      "40-45      |    23667 |    28146\n",
      "45-50      |    15382 |    19852\n",
      "50-55      |    10953 |    14001\n",
      "55-60      |     7848 |    10055\n",
      "60-65      |     5771 |     7240\n",
      "65-70      |     4033 |     5704\n",
      "70-75      |     3188 |     4180\n",
      "75-80      |     2292 |     3412\n",
      "80-85      |     1877 |     2558\n",
      "85-90      |     1429 |     2184\n",
      "90-95      |     1098 |     1746\n",
      "95-100     |      998 |     1242\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define bins (you can adjust the range and step if needed)\n",
    "bins = list(range(0, 105, 5))  # 0–5, 5–10, ..., 100+\n",
    "labels = [f\"{i}-{i+5}\" for i in bins[:-1]]\n",
    "\n",
    "# Bin the lengths\n",
    "df['en_bin'] = pd.cut(df['en_length'], bins=bins, labels=labels, right=False)\n",
    "df['hi_bin'] = pd.cut(df['hi_length'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Count frequencies\n",
    "en_dist = df['en_bin'].value_counts().sort_index()\n",
    "hi_dist = df['hi_bin'].value_counts().sort_index()\n",
    "\n",
    "# Print the distribution\n",
    "print(f\"{'Length Range':<10} | {'English':>8} | {'Hindi':>8}\")\n",
    "print(\"-\" * 32)\n",
    "for label in labels:\n",
    "    en_count = en_dist.get(label, 0)\n",
    "    hi_count = hi_dist.get(label, 0)\n",
    "    print(f\"{label:<10} | {en_count:>8} | {hi_count:>8}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5eba43-0357-48cc-8b36-2f35978b143e",
   "metadata": {},
   "source": [
    "#### Only keeping records between length 5 and 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8a7f212-da8e-4c1c-ac39-642a6fd23238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records after filtering: 815825\n",
      "                                           en_tokens  \\\n",
      "0  [give, your, application, an, accessibility, w...   \n",
      "2  [the, default, plugin, layout, for, the, botto...   \n",
      "3  [the, default, plugin, layout, for, the, top, ...   \n",
      "4  [a, list, of, plugins, that, are, disabled, by...   \n",
      "6  [the, duration, of, the, highlight, box, when,...   \n",
      "\n",
      "                                           hi_tokens  en_length  hi_length  \\\n",
      "0  [अपने, अनुप्रयोग, को, पहुंचनीयता, व्यायाम, का,...          6          8   \n",
      "2      [निचले, पटल, के, लिए, डिफोल्ट, प्लग-इन, खाका]          8          7   \n",
      "3       [ऊपरी, पटल, के, लिए, डिफोल्ट, प्लग-इन, खाका]          8          7   \n",
      "4  [उन, प्लग-इनों, की, सूची, जिन्हें, डिफोल्ट, रू...          9         12   \n",
      "6  [पहुंचनीय, आसंधि, (नोड), को, चुनते, समय, हाइला...         10         10   \n",
      "\n",
      "  en_bin hi_bin  \n",
      "0   5-10   5-10  \n",
      "2   5-10   5-10  \n",
      "3   5-10   5-10  \n",
      "4   5-10  10-15  \n",
      "6  10-15  10-15  \n"
     ]
    }
   ],
   "source": [
    "filtered_df = df[(df['en_length'] >= 5) & (df['en_length'] <= 30) &\n",
    "                 (df['hi_length'] >= 5) & (df['hi_length'] <= 30)]\n",
    "\n",
    "# Show filtered size and preview\n",
    "print(f\"Total records after filtering: {len(filtered_df)}\")\n",
    "print(filtered_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24840557-20b8-4852-a3ea-f6b92d405cf5",
   "metadata": {},
   "source": [
    "#### Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b73b6878-cbcf-4f19-ab5d-785642c808a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total duplicate records: 105443\n",
      "                                              en_str  \\\n",
      "0     give your application an accessibility workout   \n",
      "2     the default plugin layout for the bottom panel   \n",
      "3        the default plugin layout for the top panel   \n",
      "4     a list of plugins that are disabled by default   \n",
      "6  the duration of the highlight box when selecti...   \n",
      "\n",
      "                                              hi_str  \n",
      "0    अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें  \n",
      "2              निचले पटल के लिए डिफोल्ट प्लग-इन खाका  \n",
      "3               ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका  \n",
      "4  उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...  \n",
      "6  पहुंचनीय आसंधि (नोड) को चुनते समय हाइलाइट बक्स...  \n"
     ]
    }
   ],
   "source": [
    "# Make a copy to avoid SettingWithCopyWarning\n",
    "filtered_df = filtered_df.copy()\n",
    "\n",
    "# Convert token lists to strings for duplication check\n",
    "filtered_df['en_str'] = filtered_df['en_tokens'].apply(lambda x: ' '.join(x))\n",
    "filtered_df['hi_str'] = filtered_df['hi_tokens'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Find duplicates\n",
    "duplicates = filtered_df[filtered_df.duplicated(subset=['en_str', 'hi_str'], keep=False)]\n",
    "\n",
    "# Print result\n",
    "print(f\"Total duplicate records: {len(duplicates)}\")\n",
    "print(duplicates[['en_str', 'hi_str']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fca4364-7ad8-46e6-90da-8c52e22708bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(815825, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before reming duplicate\n",
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5153104a-d6e5-46f1-9906-69b17519cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping duplicates\n",
    "filtered_df = filtered_df.drop_duplicates(subset=['en_str', 'hi_str']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d5b02f2-a4d5-4879-aeab-df14369a9730",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df[['en_tokens', 'hi_tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3203c553-cfd3-46d3-ab56-4202da6f66d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(744797, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique data\n",
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45654d15-f08f-4505-b729-891547b408dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [give, your, application, an, accessibility, w...\n",
       "1     [the, default, plugin, layout, for, the, botto...\n",
       "2     [the, default, plugin, layout, for, the, top, ...\n",
       "3     [a, list, of, plugins, that, are, disabled, by...\n",
       "4     [the, duration, of, the, highlight, box, when,...\n",
       "                            ...                        \n",
       "95            [move, ~, a, onto, an, empty, top, slot.]\n",
       "96         [move, ~, a, onto, an, empty, bottom, slot.]\n",
       "97           [move, ~, a, onto, an, empty, left, slot.]\n",
       "98          [move, ~, a, onto, an, empty, right, slot.]\n",
       "99                 [move, ~, a, onto, an, empty, slot.]\n",
       "Name: en_tokens, Length: 100, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df['en_tokens'].iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c7b4325-1e6d-475e-a5f2-c95244968ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [अपने, अनुप्रयोग, को, पहुंचनीयता, व्यायाम, का,...\n",
       "1         [निचले, पटल, के, लिए, डिफोल्ट, प्लग-इन, खाका]\n",
       "2          [ऊपरी, पटल, के, लिए, डिफोल्ट, प्लग-इन, खाका]\n",
       "3     [उन, प्लग-इनों, की, सूची, जिन्हें, डिफोल्ट, रू...\n",
       "4     [पहुंचनीय, आसंधि, (नोड), को, चुनते, समय, हाइला...\n",
       "                            ...                        \n",
       "95    [~, a, को, एक, खाली, शीर्ष, स्लॉट, में, ले, जा...\n",
       "96    [~, a, को, एक, खाली, नीचे, स्लॉट, में, ले, जाएँ.]\n",
       "97    [~, a, को, एक, खाली, बाएं, स्लॉट, में, ले, जाएँ.]\n",
       "98    [~, a, को, एक, खाली, दाएँ, स्लॉट, में, ले, जाएँ.]\n",
       "99          [~, a, को, एक, खाली, स्लॉट, में, ले, जाएँ.]\n",
       "Name: hi_tokens, Length: 100, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df['hi_tokens'].iloc[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e3e980-bfd2-4fc7-90e2-5fc77fe5707f",
   "metadata": {},
   "source": [
    "#### Cleaning daa and finding vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfb9fba3-6a94-41d3-b14a-3df9c4de6b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab size: 137908\n",
      "Hindi vocab size: 246824\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Clean and build English vocab\n",
    "en_vocab_set = set()\n",
    "for tokens in filtered_df['en_tokens']:\n",
    "    # Clean each token\n",
    "    cleaned = [re.sub(r'[^a-z\\s]', '', t.lower()) for t in tokens if t]\n",
    "    cleaned = [t for t in cleaned if t]\n",
    "    en_vocab_set.update(cleaned)\n",
    "\n",
    "# Clean and build Hindi vocab\n",
    "hi_vocab_set = set()\n",
    "for tokens in filtered_df['hi_tokens']:\n",
    "    cleaned = [re.sub(r'[^\\u0900-\\u097F\\s।]', '', t) for t in tokens if t]\n",
    "    cleaned = [t for t in cleaned if t]\n",
    "    hi_vocab_set.update(cleaned)\n",
    "\n",
    "# Add special tokens and sort\n",
    "en_vocab = ['<pad>', '<sos>', '<eos>', '<unk>'] + sorted(en_vocab_set)\n",
    "hi_vocab = ['<pad>', '<sos>', '<eos>', '<unk>'] + sorted(hi_vocab_set)\n",
    "\n",
    "# Print sizes\n",
    "print(f\"English vocab size: {len(en_vocab)}\")\n",
    "print(f\"Hindi vocab size: {len(hi_vocab)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3c82d3f-58a7-489b-850f-7e5d2ad082b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<sos>', '<eos>', '<unk>', 'a', 'aa', 'aaa', 'aaad', 'aaagnf', 'aaar', 'aaarti', 'aab', 'aaber', 'aabu', 'aac', 'aacharan', 'aacharati', 'aacharya', 'aache', 'aachha', 'aachman', 'aachshaaya', 'aacr', 'aacrii', 'aactpq', 'aad', 'aadalf', 'aadam', 'aadanthy', 'aadepigmented', 'aadh', 'aadha', 'aadhaar', 'aadhaarin', 'aadhar', 'aadharbased', 'aadharlinked', 'aadheenams', 'aadhi', 'aadhunik', 'aadi', 'aadikavya', 'aadim', 'aadiparva', 'aadivasi', 'aadmi', 'aads', 'aadvise', 'aadyasevak', 'aaea', 'aaeene', 'aaeli', 'aaen', 'aag', 'aagam', 'aagamas', 'aagamshastra', 'aagara', 'aagosh', 'aagra', 'aah', 'aahamadiya', 'aahe', 'aahwan', 'aai', 'aaifr', 'aain', 'aaine', 'aainst', 'aaishwarya', 'aaj', 'aajach', 'aajad', 'aajan', 'aajeevika', 'aajkal', 'aajmagadh', 'aakaar', 'aakashdeep', 'aakasher', 'aakashwani', 'aakbari', 'aakhr', 'aakhri', 'aakhus', 'aakhusa', 'aakraneox', 'aalam', 'aalanguchchi', 'aalborg', 'aale', 'aalh', 'aali', 'aalims', 'aam', 'aamaththoor', 'aamba', 'aamer', 'aamir', 'aamlopi']\n"
     ]
    }
   ],
   "source": [
    "print(en_vocab[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4170a4a-e72c-4ff2-83ea-4654427c29f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<sos>', '<eos>', '<unk>', 'ं', 'ंं', 'ंंउ', 'ंंएम', 'ंंष्', 'ंंष्आरोग्य', 'ंंहूऍं', 'ंअइन्लिने', 'ंअकेर्', 'ंअटेर्निट्य्', 'ंअन्च्हेस्टेर्', 'ंआ', 'ंइस', 'ंईश्थ्', 'ंउस्ट्', 'ंएअस्लेस्', 'ंएडिचल्', 'ंएसोअमेरिचन', 'ंऐणीण्घीठीश्', 'ंओं', 'ंओरेचम्बे', 'ंके', 'ंकोई', 'ंजो', 'ंडऋए', 'ंड़ा', 'ंड़ी', 'ंड़े', 'ंडिया', 'ंण्', 'ंदेश', 'ंदेशे', 'ंफ्श्', 'ंबं', 'ंमम्स', 'ंमीसल्स्', 'ंमेरे', 'ंष्ठ्फ्', 'ंसूचना', 'ंसे', 'ः', 'ःंशौ', 'ःइ', 'ःइआईढ्श्', 'ःइग्हऋ', 'ःइग्ह्', 'ःऊघ्', 'ःएअल्ट्हऋ', 'ःएअल्ट्ह्', 'ःएल्प्', 'ःओउसिन्ग्', 'ःओउसे', 'ःओत्', 'ःओमे', 'ःग्', 'ःघ्', 'ःछ्', 'ःशा', 'ःशै', 'ःश्', 'ःष्फ्', 'ः।', 'अ', 'अँ', 'अँकुरित', 'अँकुरितकिण्वित', 'अँग', 'अँगड़ाई', 'अँगरेज़', 'अँगरेज़ी', 'अँगरेजी', 'अँगीठी', 'अँगुठे', 'अँगुलियाँ', 'अँगुलियां', 'अँगुलियों', 'अँगुली', 'अँगूठा', 'अँगूठियां।', 'अँगूठी', 'अँगूठे', 'अँगों', 'अँग्रज़', 'अँग्रेज', 'अँग्रेज़', 'अँग्रेज़ी', 'अँग्रेज़ीसिंधी', 'अँग्रेज़ीसिंधीकोश', 'अँग्रेज़ों', 'अँग्रेजी', 'अँग्रेजी़ः', 'अँग्रेजों', 'अँग्रेज़', 'अँग्रेज़ी', 'अँघेरा', 'अँघेरी']\n"
     ]
    }
   ],
   "source": [
    "print(hi_vocab[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "556fd163-07b7-4431-a948-69483799e10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['give', 'your', 'application', 'an', 'accessibility', 'workout'], ['अपने', 'अनुप्रयोग', 'को', 'पहुंचनीयता', 'व्यायाम', 'का', 'लाभ', 'दें']], [['the', 'default', 'plugin', 'layout', 'for', 'the', 'bottom', 'panel'], ['निचले', 'पटल', 'के', 'लिए', 'डिफोल्ट', 'प्लग-इन', 'खाका']]]\n"
     ]
    }
   ],
   "source": [
    "# converting data to [[<english tokens>,[<hindi token>]]\n",
    "parallel_data = filtered_df.apply(lambda row: [row['en_tokens'], row['hi_tokens']], axis=1).tolist()\n",
    "print(parallel_data[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622dfab4-0a93-41e6-8eca-b3d58d8ea87a",
   "metadata": {},
   "source": [
    "#### Vocab too big for us so we will use SentencePieceTrainer.bpe to generate vocab size of 30000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db4e11c8-fa5e-4063-b9e4-88ae13039450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sentencepiece as spm\n",
    "from pathlib import Path\n",
    "\n",
    "# Your data: [[english_tokens, hindi_tokens], ...]\n",
    "# Example: data = [[\"i\", \"am\", \"happy\"], [\"मैं\", \"खुश\", \"हूँ\"]]\n",
    "\n",
    "def merge(data):\n",
    "    \"\"\"Clean English and Hindi tokens in the data.\"\"\"\n",
    "    merged_data = []\n",
    "    for eng_tokens, hin_tokens in data:\n",
    "        merged_data.append([' '.join(eng_tokens), ' '.join(hin_tokens)])  # Join tokens back to sentences\n",
    "    return merged_data\n",
    "\n",
    "def train_tokenizer(cleaned_data, vocab_size=14000, retrain=False, model_prefix='en_hi'):\n",
    "    \"\"\"Train SentencePiece tokenizer and create vocab.\"\"\"\n",
    "    if retrain:\n",
    "        # Write English and Hindi to temporary files\n",
    "        with open('temp_en.txt', 'w', encoding='utf-8') as f_en, \\\n",
    "             open('temp_hi.txt', 'w', encoding='utf-8') as f_hi:\n",
    "            for eng_sent, hin_sent in cleaned_data:\n",
    "                f_en.write(eng_sent + '\\n')\n",
    "                f_hi.write(hin_sent + '\\n')\n",
    "        \n",
    "        # Train SentencePiece model\n",
    "        spm.SentencePieceTrainer.train(\n",
    "            input='temp_en.txt,temp_hi.txt',  # Combined English and Hindi\n",
    "            model_prefix=model_prefix,\n",
    "            vocab_size=vocab_size,\n",
    "            character_coverage=1.0,  # Full coverage for Hindi\n",
    "            model_type='bpe'  # Byte-Pair Encoding\n",
    "        )\n",
    "    \n",
    "    # Load the trained tokenizer\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(f'{model_prefix}.model')\n",
    "    return sp\n",
    "\n",
    "def tokenize_data(cleaned_data, tokenizer):\n",
    "    \"\"\"Tokenize cleaned data using the trained tokenizer.\"\"\"\n",
    "    tokenized_data = []\n",
    "    for eng_sent, hin_sent in cleaned_data:\n",
    "        eng_tokens = tokenizer.encode_as_pieces(eng_sent)  # Subword tokens\n",
    "        hin_tokens = tokenizer.encode_as_pieces(hin_sent)\n",
    "        tokenized_data.append([eng_tokens, hin_tokens])\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4305e65d-f8a2-4972-bed7-9e7c2dc9ffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_data = merge(parallel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "898246f7-5dab-4e25-b719-7bf2771beb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training tokenizer\n",
    "tokenizer = train_tokenizer(parallel_data, 30000,retrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "422c60a4-c169-484c-8764-0b66c2c60aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenize_data(parallel_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "842b9eef-32a3-429e-9eb6-5bdd8cd52f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['▁give', '▁your', '▁application', '▁an', '▁accessibility', '▁work', 'out'], ['▁अपने', '▁अनुप्रयोग', '▁को', '▁पहुंच', 'नीयता', '▁व्यायाम', '▁का', '▁लाभ', '▁दें']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c319e4f4-b298-44d0-a1a2-3ff1a30031e4",
   "metadata": {},
   "source": [
    "#### Adding `<bos>`,`<eos>`,`<pad>` to records, making them of 32 length. Also encoding data into ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f46e747-3194-4e06-b560-971b88217933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tokens_to_ids(tokenized_data, tokenizer, max_len=32):\n",
    "    \"\"\"Convert tokenized data to numerical IDs with padding/truncation.\"\"\"\n",
    "    numerical_data = []\n",
    "\n",
    "    vocab_size = tokenizer.get_piece_size()\n",
    "\n",
    "    pad_id = tokenizer.pad_id()\n",
    "    if pad_id == -1:\n",
    "        pad_id = 0\n",
    "\n",
    "    bos_id = tokenizer.bos_id()\n",
    "    eos_id = tokenizer.eos_id()\n",
    "\n",
    "    for eng_tokens, hin_tokens in tokenized_data:\n",
    "        # Convert tokens to IDs\n",
    "        eng_ids = tokenizer.encode_as_ids(' '.join(eng_tokens))\n",
    "        hin_ids = tokenizer.encode_as_ids(' '.join(hin_tokens))\n",
    "\n",
    "        # English: truncate to (max_len - 2), then add BOS and EOS\n",
    "        eng_ids = eng_ids[:max_len - 2]\n",
    "        eng_ids = [bos_id] + eng_ids + [eos_id]\n",
    "        eng_ids += [pad_id] * (max_len - len(eng_ids))  # Final length = 32\n",
    "\n",
    "        # Hindi: truncate to (max_len - 1), then add BOS and EOS\n",
    "        hin_ids = hin_ids[:max_len - 1]\n",
    "        hin_ids = [bos_id] + hin_ids + [eos_id]\n",
    "        hin_ids += [pad_id] * (max_len + 1 - len(hin_ids))  # Final length = 33\n",
    "\n",
    "        numerical_data.append([eng_ids, hin_ids])\n",
    "\n",
    "    return numerical_data\n",
    "\n",
    "\n",
    "def filter_by_length(numerical_data, tokenizer, min_len=10, max_len=32):\n",
    "    \"\"\"Filter out pairs where unpadded length is outside min_len to max_len.\"\"\"\n",
    "    filtered_data = []\n",
    "    pad_id = tokenizer.pad_id()\n",
    "    if pad_id == -1:\n",
    "        pad_id = 0\n",
    "\n",
    "    for eng_ids, hin_ids in numerical_data:\n",
    "        eng_len = sum(1 for id_ in eng_ids if id_ != pad_id)\n",
    "        hin_len = sum(1 for id_ in hin_ids if id_ != pad_id)\n",
    "\n",
    "        if min_len <= eng_len <= max_len and min_len <= hin_len <= (max_len + 1):\n",
    "            filtered_data.append([eng_ids, hin_ids])\n",
    "\n",
    "    return filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed7bf8e8-05fc-4f3d-9903-c150b5a845f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data = tokens_to_ids(tokenized_data, tokenizer, max_len=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e446540-e9d3-4e43-a05a-916e11377e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1707, 487, 3275, 175, 25935, 687, 596, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 354, 5509, 80, 1837, 39, 908, 61, 16873, 83, 1801, 3259, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(numerical_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0c7ed57-0797-45c5-b88d-f3c3b13f3d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert to numpy arrays (optional, for tensor feeding)\n",
    "numerical_arrays = np.array(numerical_data,dtype=object)  # Shape: (num_pairs, 2, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d654510-fe77-41f1-9858-57abb5d649a2",
   "metadata": {},
   "source": [
    "#### Saving final data as .npy file to make it reusable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cfcfb49e-3896-4b9d-8eb4-3afdb7d848ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('numerical_data.npy', numerical_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4931082-d19a-4aa2-8d36-dc9545f7d03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_arrays = np.load('numerical_data.npy', allow_pickle=True)\n",
    "filtered_numerical_data = numerical_arrays.tolist()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, temp_data = train_test_split(numerical_arrays, test_size=0.05, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "np.save('train_data.npy', train_data)\n",
    "np.save('val_data.npy', val_data)\n",
    "np.save('test_data.npy', test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "35888eb0-fe0f-434e-a535-20b5fa807bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{30: 360, 28: 418, 13: 894, 31: 2252, 12: 892, 22: 643, 7: 383, 21: 683, 25: 590, 17: 804, 29: 410, 15: 891, 20: 740, 26: 515, 18: 762, 8: 544, 14: 930, 9: 751, 11: 932, 16: 834, 24: 566, 27: 509, 19: 743, 23: 590, 10: 835, 6: 149}\n"
     ]
    }
   ],
   "source": [
    "test_data = np.load('test_data.npy', allow_pickle=True).tolist()\n",
    "lenCount = {}\n",
    "for en, hi in test_data:\n",
    "    pos = en.index(2)\n",
    "    if pos not in lenCount:\n",
    "        lenCount[pos] = 0\n",
    "    lenCount[pos] += 1\n",
    "print(lenCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1552047-edb2-42dc-ae0e-8cb4615e7bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
