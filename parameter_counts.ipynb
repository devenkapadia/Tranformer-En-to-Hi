{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42b9dbaf-48d1-452e-b4e7-ee2171f974f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_model = 128\n",
    "n_heads = 8\n",
    "dim_ff = 1024 #1st hidden layer dimention for feedforward layer\n",
    "dropout = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f67c7de0-836b-47b7-bf70-6349e1d91ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadedSelfAttention(\n",
      "  (linear_q): ModuleList(\n",
      "    (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "  )\n",
      "  (linear_k): ModuleList(\n",
      "    (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "  )\n",
      "  (linear_v): ModuleList(\n",
      "    (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "  )\n",
      "  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66048"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from encoder.multi_headed_self_attention import MultiHeadedSelfAttention\n",
    "model =  MultiHeadedSelfAttention(dim_model,n_heads)\n",
    "print(model)\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c66a6e1e-10c2-4c0a-9a9d-5ac2f88b5d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AddAndNorm(\n",
      "  (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.add_and_norm import AddAndNorm\n",
    "model = AddAndNorm(dim_model)\n",
    "print(model)\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7dd75faf-bf6e-4df9-9c75-c6430dc2dc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedForwardLayer(\n",
      "  (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "263296"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.feedforward_layer import FeedForwardLayer\n",
    "model = FeedForwardLayer(dim_model,dim_ff,dropout)\n",
    "print(model)\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e464d2ba-dd96-45ef-b010-7e4923a5f7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskedMultiHeadedSelfAttention(\n",
      "  (linear_q): ModuleList(\n",
      "    (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "  )\n",
      "  (linear_k): ModuleList(\n",
      "    (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "  )\n",
      "  (linear_v): ModuleList(\n",
      "    (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "  )\n",
      "  (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66048"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from decoder.masked_multi_headed_self_attention import MaskedMultiHeadedSelfAttention\n",
    "model = MaskedMultiHeadedSelfAttention(dim_model,n_heads)\n",
    "print(model)\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c55cc7c6-45a7-4d08-a3cb-78ae20ced4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderLayer(\n",
      "  (self_attention): MultiHeadedSelfAttention(\n",
      "    (linear_q): ModuleList(\n",
      "      (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "    )\n",
      "    (linear_k): ModuleList(\n",
      "      (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "    )\n",
      "    (linear_v): ModuleList(\n",
      "      (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "    )\n",
      "    (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (add_and_norm1): AddAndNorm(\n",
      "    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (feed_forward): FeedForwardLayer(\n",
      "    (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  )\n",
      "  (add_and_norm2): AddAndNorm(\n",
      "    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "329856"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from encoder.encoder_layer import EncoderLayer\n",
    "model = EncoderLayer(dim_model, n_heads, dim_ff)\n",
    "print(model)\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d52dae49-828a-4f95-9443-8475966d52e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecoderLayer(\n",
      "  (masked_self_attention): MaskedMultiHeadedSelfAttention(\n",
      "    (linear_q): ModuleList(\n",
      "      (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "    )\n",
      "    (linear_k): ModuleList(\n",
      "      (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "    )\n",
      "    (linear_v): ModuleList(\n",
      "      (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "    )\n",
      "    (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (add_and_norm1): AddAndNorm(\n",
      "    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (cross_attention): CrossMultiHeadedSelfAttention(\n",
      "    (linear_q): ModuleList(\n",
      "      (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "    )\n",
      "    (linear_k): ModuleList(\n",
      "      (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "    )\n",
      "    (linear_v): ModuleList(\n",
      "      (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "    )\n",
      "    (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (add_and_norm2): AddAndNorm(\n",
      "    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (feed_forward): FeedForwardLayer(\n",
      "    (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  )\n",
      "  (add_and_norm3): AddAndNorm(\n",
      "    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "396160"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from decoder.decoder_layer import DecoderLayer\n",
    "model = DecoderLayer(dim_model, n_heads, dim_ff)\n",
    "print(model)\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15a3573d-31cb-4f64-8012-2abefe94994e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (layers): ModuleList(\n",
      "    (0): EncoderLayer(\n",
      "      (self_attention): MultiHeadedSelfAttention(\n",
      "        (linear_q): ModuleList(\n",
      "          (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "        )\n",
      "        (linear_k): ModuleList(\n",
      "          (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "        )\n",
      "        (linear_v): ModuleList(\n",
      "          (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "        )\n",
      "        (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (add_and_norm1): AddAndNorm(\n",
      "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (feed_forward): FeedForwardLayer(\n",
      "        (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "      )\n",
      "      (add_and_norm2): AddAndNorm(\n",
      "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "329856"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from encoder.encoder import Encoder\n",
    "model = Encoder(dim_model, n_heads, dim_ff, 1)\n",
    "print(model)\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "624cd298-3e3b-470f-95c0-2db826dcd2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (layers): ModuleList(\n",
      "    (0): DecoderLayer(\n",
      "      (masked_self_attention): MaskedMultiHeadedSelfAttention(\n",
      "        (linear_q): ModuleList(\n",
      "          (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "        )\n",
      "        (linear_k): ModuleList(\n",
      "          (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "        )\n",
      "        (linear_v): ModuleList(\n",
      "          (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "        )\n",
      "        (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (add_and_norm1): AddAndNorm(\n",
      "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (cross_attention): CrossMultiHeadedSelfAttention(\n",
      "        (linear_q): ModuleList(\n",
      "          (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "        )\n",
      "        (linear_k): ModuleList(\n",
      "          (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "        )\n",
      "        (linear_v): ModuleList(\n",
      "          (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "        )\n",
      "        (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (add_and_norm2): AddAndNorm(\n",
      "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (feed_forward): FeedForwardLayer(\n",
      "        (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "      )\n",
      "      (add_and_norm3): AddAndNorm(\n",
      "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "396160"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from decoder.decoder import Decoder\n",
    "model = Decoder(dim_model, n_heads, dim_ff, 1)\n",
    "print(model)\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09115810-9c7f-4b71-8e07-4d48f4dc1cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EmbeddingLayer(\n",
      "  (embedding): Embedding(30000, 128)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3840000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size=30000\n",
    "from utils.embedding_layer import EmbeddingLayer\n",
    "model = EmbeddingLayer(vocab_size,dim_model)\n",
    "print(model)\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e8e8407-c5a6-4365-a3f7-1303932bf205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15805440"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2*3072000)+(5*527104)+(5*790784)+(256*12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df5a9aa5-04d9-4f3b-ba26-30fbfb44eb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): EmbeddingLayer(\n",
      "    (embedding): Embedding(30000, 128)\n",
      "  )\n",
      "  (1): EmbeddingLayer(\n",
      "    (embedding): Embedding(30000, 128)\n",
      "  )\n",
      "  (2): Transformer(\n",
      "    (encoder): Encoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): EncoderLayer(\n",
      "          (self_attention): MultiHeadedSelfAttention(\n",
      "            (linear_q): ModuleList(\n",
      "              (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "            )\n",
      "            (linear_k): ModuleList(\n",
      "              (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "            )\n",
      "            (linear_v): ModuleList(\n",
      "              (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "            )\n",
      "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (add_and_norm1): AddAndNorm(\n",
      "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "          )\n",
      "          (add_and_norm2): AddAndNorm(\n",
      "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): Decoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): DecoderLayer(\n",
      "          (masked_self_attention): MaskedMultiHeadedSelfAttention(\n",
      "            (linear_q): ModuleList(\n",
      "              (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "            )\n",
      "            (linear_k): ModuleList(\n",
      "              (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "            )\n",
      "            (linear_v): ModuleList(\n",
      "              (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "            )\n",
      "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (add_and_norm1): AddAndNorm(\n",
      "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (cross_attention): CrossMultiHeadedSelfAttention(\n",
      "            (linear_q): ModuleList(\n",
      "              (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "            )\n",
      "            (linear_k): ModuleList(\n",
      "              (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "            )\n",
      "            (linear_v): ModuleList(\n",
      "              (0-7): 8 x Linear(in_features=128, out_features=16, bias=True)\n",
      "            )\n",
      "            (linear_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (add_and_norm2): AddAndNorm(\n",
      "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (feed_forward): FeedForwardLayer(\n",
      "            (linear1): Linear(in_features=128, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "          )\n",
      "          (add_and_norm3): AddAndNorm(\n",
      "            (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc_out): Linear(in_features=128, out_features=30000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12276016"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "import torch\n",
    "from transformer import Transformer\n",
    "vocab_size1=30000 #137908\n",
    "vocab_size2=30000 #246824\n",
    "dim_model = 128\n",
    "n_heads = 8\n",
    "dim_ff = 1024\n",
    "dropout = 0.1\n",
    "decoder = Decoder(dim_model, n_heads, dim_ff, 1)\n",
    "encoder = Encoder(dim_model, n_heads, dim_ff, 1)\n",
    "inEmb = EmbeddingLayer(vocab_size1,dim_model)\n",
    "outEmb = EmbeddingLayer(vocab_size2,dim_model)\n",
    "transformer = Transformer(encoder, decoder, dim_model, vocab_size2)\n",
    "seqModel = torch.nn.Sequential(\n",
    "    inEmb,\n",
    "    outEmb,\n",
    "    transformer\n",
    ")\n",
    "    \n",
    "print(seqModel)\n",
    "#print(transformer)\n",
    "sum(p.numel() for p in seqModel.parameters() if p.requires_grad)\n",
    "#sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e6a8a8-a64e-497b-ae49-147e7911f290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
